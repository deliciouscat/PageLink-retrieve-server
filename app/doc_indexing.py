"""
ë¬¸ì„œ ì¸ë±ì‹± ëª¨ë“ˆ - pydantic-ai êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©
"""
import os
import asyncio
import random
from app.llm.inference import structured_inference, inference
from app.llm.parser import json_parser, safe_json_parser
from app.data_model import QuestionsResponse
from jinja2 import Environment, FileSystemLoader
import json
from pydantic import BaseModel, Field
from typing import List


# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ templates í´ë” ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(current_dir, 'llm')
env = Environment(loader=FileSystemLoader(template_dir))

# ê¸°ì¡´ str_to_json í•¨ìˆ˜ëŠ” parser ëª¨ë“ˆë¡œ ì´ë™
# from app.llm.parser import json_parser as str_to_json  # í•˜ìœ„ í˜¸í™˜ì„±

class Question(BaseModel):
    """ìƒì„±ëœ ì§ˆë¬¸ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸"""
    question: str = Field(description="ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸")
    answer: str = Field(description="ìƒì„±ëœ ì§ˆë¬¸ì˜ ë‹µë³€")
    

class QuestionsResponse(BaseModel):
    """ì—¬ëŸ¬ ì§ˆë¬¸ë“¤ì„ ë‹´ëŠ” ì‘ë‹µ ëª¨ë¸"""
    questions: List[Question] = Field(description="ìƒì„±ëœ ì§ˆë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸", min_items=2, max_items=6)

async def doc_indexing(data_instance):
    """
    ë¬¸ì„œ ì¸ë±ì‹± í•¨ìˆ˜ - pydantic-ai êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©
    doc_inputì„ ë°›ì•„ì„œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±
    """
    print(f"ğŸ” [doc_indexing] ì‹œì‘ - User: {data_instance.user_id}")
    
    template = env.get_template('prompts/doc_indexing_250830.jinja')
    system_prompt = template.render(doc_input=data_instance.doc_input, memopad=data_instance.collection_memo)
    
    try:
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ìƒˆë¡œìš´ inference í•¨ìˆ˜ ì‚¬ìš©
        result = await structured_inference(
            prompt=data_instance.doc_input,
            model_name="x-ai/grok-3-mini",
            model_settings={
                "temperature": 0.75,
                "max_tokens": 1000,
            },
            system_prompt=system_prompt,
            output_type=QuestionsResponse  # êµ¬ì¡°í™”ëœ ì¶œë ¥ íƒ€ì… ì§€ì •
        )
        
        # raw result ì¶œë ¥
        print(result)
        
        # result.outputì´ ì´ë¯¸ QuestionsResponse ê°ì²´ì„
        questions_response = result.output
        
        print(f"âœ… [doc_indexing] ì™„ë£Œ - User: {data_instance.user_id}")

        
        return {
            "questions": [q.model_dump() for q in questions_response.questions]
        }
        
    except Exception as e:
        print(f"âš ï¸ [doc_indexing] êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback: {e}")
        
        # fallback: ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°©ì‹
        result = await inference(
            prompt=data_instance.doc_input,
            model_name="x-ai/grok-3-mini",
            model_settings={
                "temperature": 0.75,
                "max_tokens": 1000,
            },
            system_prompt=system_prompt
        )
        
        # ì•ˆì „í•œ JSON íŒŒì‹± ì‚¬ìš©
        questions = safe_json_parser(result.output, QuestionsResponse)
        
        if questions:
            print(f"âœ… [doc_indexing] Fallback ì™„ë£Œ - User: {data_instance.user_id}")
            return {
                "questions": [q.model_dump() for q in questions.questions]
            }
        else:
            # ë§ˆì§€ë§‰ fallback: ê¸°ë³¸ JSON íŒŒì‹±
            basic_questions = safe_json_parser(result.output)
            print(f"âš ï¸ [doc_indexing] ê¸°ë³¸ íŒŒì‹±ìœ¼ë¡œ ì™„ë£Œ - User: {data_instance.user_id}")
            return basic_questions or {"questions": [], "total_count": 0}
