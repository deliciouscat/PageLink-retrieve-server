"""
ì»¬ë ‰ì…˜ ì¿¼ë¦¬ í™•ì¥ ëª¨ë“ˆ
"""
import os
import asyncio
import random
from app.llm.inference import inference#structured_inference
from jinja2 import Environment, FileSystemLoader
from pydantic import BaseModel, Field
from typing import List
import json


# í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ templates í´ë” ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
template_dir = os.path.join(current_dir, 'llm')
env = Environment(loader=FileSystemLoader(template_dir))

class Question(BaseModel):
    """ìƒì„±ëœ ì§ˆë¬¸ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨ë¸"""
    question: str = Field(description="ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ ì§ˆë¬¸")
    #answer: str = Field(description="ìƒì„±ëœ ì§ˆë¬¸ì˜ ë‹µë³€")
    

class QuestionsResponse(BaseModel):
    """ì—¬ëŸ¬ ì§ˆë¬¸ë“¤ì„ ë‹´ëŠ” ì‘ë‹µ ëª¨ë¸"""
    questions: List[Question] = Field(description="ìƒì„±ëœ ì§ˆë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸", min_items=2, max_items=6)

async def expand_collection_query(data_instance):
    """
    ë¬¸ì„œ ì¸ë±ì‹± í•¨ìˆ˜ - pydantic-ai êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©
    doc_inputì„ ë°›ì•„ì„œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±
    """
    print(f"ğŸ” [expand_collection_query] ì‹œì‘ - User: {data_instance.user_id}")
    
    template = env.get_template('prompts/expand_collection_query_250903.jinja')
    # doc_summaries êµ¬ì„±
    data_instance.doc_summarized.append({"summary": data_instance.doc_summarized_new, "summary_id": data_instance.doc_summarized_new_id})
    doc_summaries_joined = "\n------\n".join([summary["summary"] for summary in data_instance.doc_summarized])
    # ì—¬ê¸°ê¹Œì§„ í™•ì‹¤íˆ ë¨
    prompt = template.render(doc_summaries=doc_summaries_joined, collection_memo=data_instance.collection_memo)
    
    # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ ìƒˆë¡œìš´ inference í•¨ìˆ˜ ì‚¬ìš©
    result = await inference(
        prompt=prompt,
        # structured outputì€ ì œí•œëœ ëª¨ë¸ë§Œ ê°€ëŠ¥:
        # - gpt-4 ê³„ì—´, gemini-2.5 ê³„ì—´
        # - ë¶ˆê°€ëŠ¥í•œ ëª¨ë¸: gpt-5 ê³„ì—´, grok-3 ê³„ì—´
        #model_name="google/gemini-2.5-pro",
        #model_name="x-ai/grok-3-mini",
        model_name="qwen/qwen3-235b-a22b-thinking-2507",
        model_settings={
            "temperature": 0.7,
            "max_tokens": 5000,
        },
        system_prompt=None,
        #output_type=QuestionsResponse  # êµ¬ì¡°í™”ëœ ì¶œë ¥ íƒ€ì… ì§€ì •
    )
    questions_response = json.loads(result.output)
    #print("ğŸ”ğŸ”questions_responseğŸ”ğŸ”")
    #print(questions_response)
    print(f"âœ… [doc_indexing] ì™„ë£Œ - User: {data_instance.user_id}")

    return questions_response

    # structured output ì‚¬ìš© ì‹œ ì½”ë“œ
    #return {
    #    "questions": [q.model_dump() for q in questions_response.questions]
    #}
    